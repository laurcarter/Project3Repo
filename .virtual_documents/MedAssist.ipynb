import pandas as pd
import numpy as np
import seaborn as sns

import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots

import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')


df = pd.read_csv("medquad.csv")
df = df.dropna()

#df = df.sample(2000)
#df = df.reset_index()
#df = df.drop('index',axis =1)
display(df.shape)
df.head()


import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'df' is your input DataFrame
focus_area_counts = df['focus_area'].value_counts()
focus_areas = focus_area_counts[focus_area_counts > 15].index

fig, ax = plt.subplots(figsize=(20, 8))
sns.set(font_scale=1.2)

ax.pie(focus_area_counts[focus_areas], labels=focus_areas, autopct='%1.1f%%')
ax.set_title('Pie Chart of Focus Areas')
ax.axis('equal')  # Equal aspect ratio ensures that pie is circular.

plt.show()


import random


# ANSI color codes

color_codes = {

    "blue": 34,
    "green": 32,
    "red": 31,
    "purple": 35,
    "orange": 33,
    "yellow": 33,
    "pink": 35,
    "brown": 33,
    "gray": 37
}



for i in range(0, len(df), 7):

    color = random.choice(list(color_codes.values()))

    print(f"\033[1;{color}mThe question is: {df['question'][i]}\033[0m\n\033[1;{color}m The answer is: {df['answer'][i]}\033[0m\n")

    if i > 30:

        break


!pip install transformers sentence-transformers faiss-cpu


from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

# Question Encoder (for retrieving documents)
question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')
question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')

# Context Encoder (for encoding the documents in the knowledge base)
context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')
context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')


!pip install tf-keras
from transformers import BartForConditionalGeneration, BartTokenizer
from sentence_transformers import SentenceTransformer
import torch

# Generator model
generator = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
generator_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')


unique_focus_areas = df['focus_area'].unique()

# Filter for focus areas with more than 50 occurrences
filtered_focus_areas = df['focus_area'].value_counts()[df['focus_area'].value_counts() > 15]


# Get the top 1000
selected_focus_areas = filtered_focus_areas.head(1000)

# Print the selected focus areas
selected_focus_areas

new_df = pd.DataFrame()

for focus_area in selected_focus_areas.index:
    # Filter the original DataFrame for the current focus area
    focus_area_df = df[df['focus_area'] == focus_area]

    # Sample 20 rows from the filtered DataFrame
    sampled_df = focus_area_df.sample(n=min(20, len(focus_area_df))) # Ensure we don't try to sample more than exist

    # Concatenate the sampled rows to the new DataFrame
    new_df = pd.concat([new_df, sampled_df])

# Now 'new_df' contains up to 20 samples from each of the selected focus areas.
display(new_df.shape)

new_df.head()


 encoded_docs = []
for index, row in new_df.iterrows():

    inputs = context_tokenizer(row['answer'], return_tensors='pt', truncation=True, max_length=512)

    with torch.no_grad():

        doc_embedding = context_encoder(**inputs).pooler_output

    encoded_docs.append(doc_embedding.numpy())


import faiss

import numpy as np



encoded_docs = np.array([embedding.reshape(1,-1) for embedding in encoded_docs]) # Reshape each embedding to (1, embedding_dimension)

encoded_docs = np.vstack(encoded_docs) # Stack the reshaped embeddings into a single array



# Now proceed with indexing

dimension = encoded_docs.shape[1]  # Get the embedding dimension

index = faiss.IndexFlatIP(dimension)  # Using Inner Product (dot-product) for similarity

index.add(encoded_docs)





%%time


def rag(query):

    # Tokenize the question

    question_inputs = question_tokenizer(query, return_tensors='pt')

    with torch.no_grad():

        # Embeddings

        question_embedding = question_encoder(**question_inputs).pooler_output.numpy()

    # Retrieve indexes of top 2 documents

    top_k = 2

    _, indices = index.search(question_embedding, top_k)

    retrieved_docs = [new_df['answer'].iloc[idx] for idx in indices[0]] # Use new_df and iloc

    # Concatenate retrieved documents with the query as input for the generator

    context = " ".join(retrieved_docs)

    input_text = query + " " + context

    # Generate a response using the generator model

    inputs = generator_tokenizer(input_text, return_tensors="pt", max_length=1024, truncation=True)

    output_ids = generator.generate(**inputs, max_length=20, num_beams=5, early_stopping=True)

    answer = generator_tokenizer.decode(output_ids[0], skip_special_tokens=True)

    return answer


new_df['focus_area'].value_counts()


import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'df' is your input DataFrame
focus_area_counts =new_df['focus_area'].value_counts()
focus_areas = focus_area_counts[focus_area_counts > 15].index

fig, ax = plt.subplots(figsize=(20, 8))
sns.set(font_scale=1.2)

ax.pie(focus_area_counts[focus_areas], labels=focus_areas, autopct='%1.1f%%')
ax.set_title('Pie Chart of Focus Areas')
ax.axis('equal')  # Equal aspect ratio ensures that pie is circular.

plt.show()


query = "What is the Breast Cancer?"

response = rag(query)

print("Question:", query)
print("Response:", response)


query = "What is COPD?"

response = rag(query)

print("Question:", query)
print("Response:", response)


query = "What is Osteoporosis?"

response = rag(query)

print("Question:", query)
print("Response:", response)


query = "What is High Blood Pressure?"

response = rag(query)


print(f"\033[1;31mQuestion:\033[0m \033[1;36m{query}\033[0m")
print(f"\033[1;31mResponse:\033[0m \033[1;36m{response}\033[0m")
